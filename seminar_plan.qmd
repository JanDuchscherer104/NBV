---
title: "Seminar Project: Active 3D Reconstruction and Human–Machine Collaboration"
author: "Jan Duchscherer"
date: "today()"
format: html
bibliography: references.bib
---

# Literature overview

Recent work in active 3‑D reconstruction highlights both the promise and the challenges of automating data capture.  Manual scanning of large environments is **time‑consuming and error‑prone**; for example, scanning a building‑scale scene with a drone can take several days for a professional team, and even experts can overlook areas that require multiple rescans [@chen2024gennbv].  Next‑best‑view (NBV) policies attempt to automate view planning by repeatedly proposing viewpoints that maximise information gain.  **Classical NBV methods** rely on hand‑crafted rules and restricted action spaces (e.g. selecting camera poses from a hemisphere), which limits their generalisation to new scenes [@chen2024gennbv].

To address these limitations, **learning‑based NBV policies** have emerged. The **GenNBV** framework models NBV selection as a reinforcement‑learning problem with a 5‑D continuous action space. It uses a multi‑source state embedding that combines geometric, semantic and action information and is trained using the Isaac Gym simulator on Houses3K and OmniObject3D datasets.  GenNBV achieves **coverage ratios of 98.26 % and 97.12 % on unseen buildings**, demonstrating strong cross‑dataset generalisation [@chen2024gennbv].

Another line of research focuses on the computational cost of NBV planning.  **PB‑NBV** (Projection‑Based NBV) replaces costly ray‑casting with an ellipsoid‑based voxel representation and a projection‑based viewpoint‑quality function.  This **reduces computation and maintains high point‑cloud coverage**, outperforming prior frameworks both in simulation and real‑world experiments.

**Semantics‑aware NBV** planners incorporate object or region importance into view selection.  They combine visibility‑based information gain with a **semantic gain** term so that the system prioritises task‑relevant objects (e.g. plants to harvest) and avoids wasting time on irrelevant parts.  Such strategies have been shown to detect more relevant objects with fewer viewpoints than purely geometric approaches.

The **SceneScript** language and model present a complementary direction. SceneScript represents complete scene layouts as structured language commands generated by a transformer [@avetisyan2024scenescript].  The recent **Human‑in‑the‑Loop Local Corrections via Infilling** paper introduces a system where users can click on erroneous parts of a layout and have the model “infilling” those regions.  The authors train a multi‑task version of SceneScript that maintains global prediction quality while **significantly improving local correction capability** [@xie2025hilscenescript].  The model is trained on **Aria Synthetic Environments** (ASE), a dataset of 100 k synthetic Manhattan‑world indoor scenes with egocentric walkthroughs, semi‑dense point‑clouds and ground‑truth layouts [@xie2025hilscenescript].  Their human‑in‑the‑loop workflow allows iterative refinements and enables the final layout to diverge from the training distribution, producing more accurate reconstructions [@xie2025hilscenescript].

# Roadmap

<!-- - REVISE: from today() to second week of January 2026 -->
- Goal for the seminar: implement a system that wraps relevant entities in the scene in suited primitives (e.g. OOBs or convex hulls)
- From these primitives, it should be possible to derive a maps to capture:
  - semantic calue of entities
  - coverage of these entities from different viewpoints

However, this approach of starting with a coarse scene representation from which the semantic relevance, coverage map and RRI (relative reconstruction improvement) of new views can be derived is only one possible direction.



1. **Phase 1 – Baseline mapping and coverage** (Weeks 1–4):  \
   • Select a SLAM/reconstruction backend (e.g. RTAB‑Map, Voxblox) and implement a mapping pipeline.  \
   • Develop a coverage‑analysis module that identifies unscanned regions and visualises them as a heatmap.  \
   • Prepare a Quarto notebook to document datasets and evaluation metrics.

2. **Phase 2 – NBV planning** (Weeks 5–7):  \
   • Implement a simple NBV planner based on visibility/information gain.  \
   • Benchmark PB‑NBV and GenNBV on synthetic and real scenes; analyse performance trade‑offs in terms of computation and coverage.  \
   • Document the results and lessons learned.

3. **Phase 3 – Semantic integration** (Weeks 8–10):  \
   • Add a semantic segmentation module (e.g. Segment‑Anything, Mask2Former) to label objects in the map.  \
   • Design a semantic‑aware NBV planner using a combined visibility and semantic‑gain metric.  \
   • Evaluate how semantic cues affect coverage and mapping efficiency.

4. **Phase 4 – Human‑in‑the‑loop and language models** (Weeks 11–13):  \
   • Integrate an LLM (e.g. GPT‑4, Llama‑3) to answer user questions about map coverage and propose high‑level plans.  \
   • Implement a “one‑click fix” interface inspired by SceneScript’s local correction system [@xie2025hilscenescript].  \
   • Design prompts that allow the LLM to suggest NBV adjustments or explain mapping status.

5. **Phase 5 – Seminar deliverables and thesis planning** (Weeks 14–16):  \
   • Prepare a seminar presentation summarising methods, experiments and findings.  \
   • Refine research questions based on the seminar results and outline the scope of the Master’s thesis.  \
   • Identify remaining challenges (e.g. real‑time constraints, robust semantic segmentation) to be addressed in the thesis.

# Ideas and research questions

The project aims to push beyond classical NBV planning by incorporating semantics and human guidance.  Example research questions include:

- **Semantic importance:** How can semantic labels (e.g. walls, doors, tables) guide NBV selection?  Can semantic gain be weighted by task relevance (as in plant‑specific NBV)?
- **Human‑in‑the‑loop planning:** What is the optimal way to combine automated NBV proposals with sparse human corrections?  Can local corrections be framed as an infilling problem similar to the SceneScript approach [@xie2025hilscenescript]?
- **LLM integration:** How can language models translate coverage maps and semantic annotations into natural language explanations or high‑level navigation strategies?  Could the LLM itself suggest NBV poses based on scene summaries?
- **Transfer learning:** How well do policies trained in synthetic environments (ASE, Houses3K) generalise to real‑world scans?  What domain‑adaptation techniques are needed to bridge the gap?
- **Efficiency:** Can projection‑based methods (PB‑NBV) provide sufficient speed on embedded hardware while maintaining good coverage?
- **Multi‑modal data:** How does integrating other modalities (audio, inertial data) impact mapping quality and NBV planning?

# General overview of the repository and our work

Our previous project, **3D Spatial Scene Understanding and Interactive Audio Guidance for Blind Users**, aimed to build an application that provides depth estimation, object detection and interactive audio guidance for navigation.  It followed a modular pipeline comprising data parsing, segmentation, scene understanding and an AI‑powered live agent built with the Gemini API.  The repository included configuration classes (based on Pydantic), data contracts, a live agent interface and a Streamlit UI for real‑time interaction.  This new seminar project will reuse the **architecture patterns and tooling** while shifting the focus to **active 3‑D reconstruction and NBV planning**.  The goal is to develop a general framework that can:

- Maintain a continuously updated 3‑D map (point‑cloud, voxel grid or implicit representation).
- Analyse coverage and identify unscanned regions.
- Plan the next best viewpoints, incorporating semantic cues and human feedback.
- Provide an interface (CLI, AR overlay or chat) for users to query map status and request corrections.
- Integrate with large language models for natural language explanation and high‑level planning.

The repository will be structured according to **modular and configuration‑driven principles** (see Agent instructions).  Each stage of the pipeline (mapping, coverage analysis, NBV planning, semantic segmentation, LLM interaction) will be encapsulated in a separate module with clearly defined input/output data contracts.

# Questions for our supervisor

- **Scope and deliverables:** What level of functionality is expected for the seminar project (e.g. working NBV planner vs. full human‑in‑the‑loop system)?
- **Dataset permissions:** Are there restrictions on using synthetic datasets (ASE, Houses3K) or collecting custom scans on campus?
- **Hardware access:** Will GPU resources be available for training RL models and running semantic segmentation?
- **Evaluation criteria:** Which metrics (coverage, F1, user study) should we prioritise in the seminar vs. the thesis?
- **Integration with LLMs:** Is it acceptable to use external APIs (e.g. OpenAI, Gemini) for language‑model components, or should we rely on open‑source models?
- **Collaboration with other teams:** Are there related projects within the lab that we should coordinate with (e.g. work on semantic segmentation or robotics)?

# Agent instructions

Codex (or any future coding agent) should adhere to the following **coding guidelines** when working on this repository:

- **Modular pipeline:** Design the codebase as a sequence of independent, replaceable stages.  Each stage should define its inputs and outputs via typed data contracts.
- **Configuration‑driven design:** Use strongly‑typed configuration classes (e.g. Pydantic models) as factories for runtime components.  All hyperparameters, file paths and model choices should be specified in config objects; runtime classes must not perform validation.
- **Config‑as‑Factory pattern:** Provide a `setup_target()` method on each config class to instantiate and validate the corresponding runtime object.  Avoid hard‑coded dependencies; instead, inject dependencies via nested config objects.
- **Singletons for global state:** Use singleton patterns only for truly global resources (paths, environment variables).  Provide accessors for these resources rather than referencing static variables directly.
- **Model selection via nested configs:** Avoid enums or string flags for selecting algorithms.  Instead, allow config classes to accept union types of other config classes (e.g. `model_config: Union[PB_NBV_Config, GenNBV_Config]`).
- **Type annotations and docstrings:** Annotate all functions and methods with explicit types.  Document public APIs using concise docstrings that specify tensor shapes and dtypes.  For example:

  ```python
  def forward(self, src_seq: Tensor["B N F", float32],
                    tgt_seq: Tensor["B N F", float32],
                    memory: Tensor["B N F", float32]) -> Tuple[Tensor, Tensor]:
      """
      Forward pass for the NBV policy network.

      Args:
          src_seq: Sequence of encoded features for N candidate views.
          tgt_seq: Target sequence of position embeddings.
          memory: Encoder memory tensor.

      Returns:
          Tuple of (decoder_output, updated_memory) tensors.
      """
      ...
  ```

- **Logging and error handling:** Centralise logging (e.g. via the `rich` console) and ensure that all warnings, errors and fallbacks are recorded.  Do not embed validation logic inside runtime classes; use config validators instead.
- **No hardcoded paths:** All file system paths must come from configuration objects, validated at initialisation.
- **Functional style:** Prefer functional programming constructs or vectorised operations to explicit loops when manipulating tensors or arrays.
- **Extensibility:** Design interfaces such that new modules (e.g. different NBV planners, different reconstruction backends) can be plugged in without modifying existing code.
- **Documentation:** Keep this Quarto document updated as the project evolves; include experiment results, decisions and open issues.

# Style guide

For prose and documentation within this repository:

- Use clear, concise language.  Each paragraph should focus on a single idea and be 3–5 sentences long.
- Organise content with logical headings (using `#`, `##`, etc.) and bullet lists for grouped ideas.  Avoid long paragraphs or nested lists.
- When citing papers or datasets, provide inline citations using the tether notation (e.g. `【800928672134040†L86-L90】`).
- Place tables only when necessary (e.g. to summarise metrics); avoid long sentences inside tables.
- Ensure that figures (e.g. coverage heatmaps or NBV trajectories) include descriptive captions and are referenced in the text.
- Use Quarto’s built‑in capabilities (e.g. callouts, code blocks) to improve readability.
- Maintain a professional tone throughout; avoid colloquial language and filler.

This document is intended as a living plan for the seminar project and the upcoming Master’s thesis.  Update it regularly with new insights, experimental results and refined research questions.